# XIDS - Explainable Intrusion Detection System
## Updated Project Structure

This document describes the reorganized XIDS project structure for improved maintainability, scalability, and code organization.

## ğŸ“ Project Structure Overview

```
XIDS/
â”œâ”€â”€ backend/                          # ML Backend
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw/                      # Raw datasets
â”‚   â”‚   â”‚   â”œâ”€â”€ KDDTrain+.txt
â”‚   â”‚   â”‚   â””â”€â”€ KDDTest+.txt
â”‚   â”‚   â””â”€â”€ processed/                # Processed data
â”‚   â”‚       â”œâ”€â”€ train_processed.csv
â”‚   â”‚       â””â”€â”€ test_processed.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/                # Data Processing Pipeline
â”‚   â”‚   â”œâ”€â”€ load_data.py              # Data loading
â”‚   â”‚   â”œâ”€â”€ clean_data.py             # Data cleaning
â”‚   â”‚   â”œâ”€â”€ encode_normalize.py       # Encoding & normalization
â”‚   â”‚   â””â”€â”€ preprocess_pipeline.py    # Orchestration
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                     # Feature Engineering
â”‚   â”‚   â”œâ”€â”€ feature_selection.py      # Feature selection methods
â”‚   â”‚   â””â”€â”€ feature_analysis.py       # Feature analysis
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                       # Model Training & Evaluation
â”‚   â”‚   â”œâ”€â”€ train_model.py            # Training pipeline
â”‚   â”‚   â”œâ”€â”€ evaluate_model.py         # Evaluation metrics
â”‚   â”‚   â””â”€â”€ saved_models/
â”‚   â”‚       â”œâ”€â”€ model.pkl
â”‚   â”‚       â””â”€â”€ preprocessor.pkl
â”‚   â”‚
â”‚   â”œâ”€â”€ explainability/               # Model Explainability
â”‚   â”‚   â”œâ”€â”€ shap_explainer.py         # SHAP explanations
â”‚   â”‚   â”œâ”€â”€ lime_explainer.py         # LIME explanations
â”‚   â”‚   â””â”€â”€ rule_extraction.py        # Rule extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                        # Utilities
â”‚   â”‚   â””â”€â”€ config.py                 # Configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ app/                          # FastAPI Application
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/                   # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/                 # Business logic
â”‚   â”‚   â””â”€â”€ schemas/                  # Request/response schemas
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ main.py                       # Backend entry point
â”‚
â”œâ”€â”€ frontend/                          # Streamlit Frontend
â”‚   â”œâ”€â”€ app.py                        # Main application
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ sidebar.py               # Navigation
â”‚   â”‚   â”œâ”€â”€ prediction_view.py       # Predictions
â”‚   â”‚   â”œâ”€â”€ explanation_view.py      # Explanations
â”‚   â”‚   â””â”€â”€ login.py                 # Authentication
â”‚   â”‚
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â””â”€â”€ styles.css               # Styling
â”‚   â”‚
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â””â”€â”€ logo.png                 # Branding
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .users.json                  # User database
â”‚
â”œâ”€â”€ results/                           # Output Results
â”‚   â”œâ”€â”€ metrics/                      # Model metrics
â”‚   â”œâ”€â”€ plots/                        # Visualizations
â”‚   â””â”€â”€ explanations/                 # Explanations
â”‚
â”œâ”€â”€ notebooks/                         # Analysis Notebooks
â”‚   â”œâ”€â”€ data_exploration.ipynb       # Data analysis
â”‚   â””â”€â”€ feature_importance.ipynb     # Feature analysis
â”‚
â”œâ”€â”€ docker-compose.yml                # Docker orchestration
â”œâ”€â”€ requirements.txt                  # Root dependencies
â”œâ”€â”€ PROJECT_STRUCTURE.md              # Structure documentation
â”œâ”€â”€ README.md                         # Project guide
â””â”€â”€ .gitignore
```

## ğŸš€ Key Components

### Backend Modules

#### 1. **Preprocessing** (`backend/preprocessing/`)
Handles data loading, cleaning, and transformation.

```python
from preprocessing.preprocess_pipeline import preprocess_pipeline

# Complete pipeline execution
X_train, X_test, y_train, y_test = preprocess_pipeline(
    filepath='data/raw/KDDTrain+.txt',
    target_column='Label',
    test_size=0.2,
    normalization_method='standard'
)
```

**Key Functions:**
- `load_data()`: Load CSV/TXT files
- `clean_data()`: Remove duplicates, handle missing values
- `encode_categorical_features()`: Encode categorical columns
- `normalize_numeric_features()`: Scale features
- `preprocess_pipeline()`: Complete workflow

#### 2. **Features** (`backend/features/`)
Feature selection and analysis.

```python
from features.feature_selection import select_kbest_features, get_feature_importance_scores
from features.feature_analysis import analyze_feature_distribution, detect_outliers

# Feature selection
selected_indices = select_kbest_features(X, y, k=20)

# Feature analysis
importance_scores = get_feature_importance_scores(X, y)
distribution = analyze_feature_distribution(X)
outliers = detect_outliers(X, threshold=3.0)
```

**Key Functions:**
- `select_kbest_features()`: K-best feature selection
- `select_mutual_information_features()`: MI-based selection
- `select_forest_features()`: RF importance selection
- `compute_feature_statistics()`: Statistical analysis
- `analyze_class_distribution()`: Class imbalance analysis

#### 3. **Models** (`backend/models/`)
Model training, evaluation, and persistence.

```python
from models.train_model import XIDSTrainer

# Training
trainer = XIDSTrainer()
metrics = trainer.train_pipeline('data/processed/train_processed.csv')

# Evaluation
from models.evaluate_model import evaluate_model
results = evaluate_model(y_test, y_pred)
```

**Key Classes/Functions:**
- `XIDSTrainer`: Main training orchestrator
- `train_xgboost()`: XGBoost training
- `train_random_forest()`: Random Forest training
- `evaluate_model()`: Compute metrics
- `compare_models()`: Model comparison

#### 4. **Explainability** (`backend/explainability/`)
Model interpretation and explanation.

```python
from explainability.shap_explainer import SHAPExplainer
from explainability.lime_explainer import LIMEExplainer

# SHAP explanations
shap_exp = SHAPExplainer(model, X_train)
explanation = shap_exp.explain_instance(X_instance)

# LIME explanations
lime_exp = LIMEExplainer(model)
local_exp = lime_exp.explain_prediction(X_instance, num_features=10)
```

**Key Classes/Functions:**
- `SHAPExplainer`: SHAP-based explanations
- `LIMEExplainer`: LIME local explanations
- `RuleExtractor`: Decision rule extraction

#### 5. **Configuration** (`backend/utils/`)
Centralized configuration management.

```python
from utils.config import (
    MODEL_CONFIG, FEATURE_CONFIG, DATA_CONFIG,
    MODELS_ROOT, DATA_ROOT, RESULTS_ROOT
)

# Access configuration
model_params = MODEL_CONFIG['xgboost']
data_dir = DATA_ROOT / 'raw'
```

### Frontend Features

#### Authentication
- Email/password registration
- Login/logout functionality
- Session state management
- User database (JSON)

#### Dark Cybersecurity Theme
- Primary dark: #0D1117
- Accent green: #00FF41
- Accent cyan: #00CED1
- Glow effects and gradients

#### Components
- **Sidebar**: Navigation and user info
- **Prediction View**: Input form and results
- **Explanation View**: SHAP/LIME visualizations
- **Login Page**: User authentication

## ğŸ“Š Data Pipeline

### Processing Workflow

```
Raw Data (KDDTrain+.txt, KDDTest+.txt)
    â†“ load_data.py
Load & Validate
    â†“ clean_data.py
Remove Duplicates & Missing Values
    â†“ encode_normalize.py
Encode Categorical & Normalize Numeric
    â†“ split_data
Train/Test Split (80/20)
    â†“ preprocess_pipeline.py
Processed Data (train_processed.csv, test_processed.csv)
```

## ğŸ¤– Model Training Pipeline

```
Processed Data
    â†“ feature_selection.py
Feature Engineering
    â†“ train_model.py
â”œâ”€ XGBoost Training
â”œâ”€ Random Forest Training
â””â”€ Model Selection
    â†“ evaluate_model.py
Evaluation Metrics
    â†“ saved_models/
model.pkl & preprocessor.pkl
```

## ğŸ” Explainability Pipeline

```
Model Prediction
    â†“
â”œâ”€ shap_explainer.py
â”‚  â””â”€ SHAP Values & Feature Importance
â”œâ”€ lime_explainer.py
â”‚  â””â”€ Local Linear Approximation
â””â”€ rule_extraction.py
   â””â”€ Decision Rules
    â†“
Explanation Output
```

## ğŸ› ï¸ Installation & Setup

### 1. Environment Setup
```bash
cd XIDS
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Data Preparation
Place datasets in `backend/data/raw/`:
- `KDDTrain+.txt`
- `KDDTest+.txt`

### 3. Model Training
```bash
cd backend/models
python train_model.py
```

### 4. Start Backend
```bash
cd backend
python -m uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### 5. Start Frontend
```bash
cd frontend
streamlit run app.py
```

### 6. Access Application
- **Frontend**: http://localhost:8501
- **API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs

## ğŸ“ Usage Examples

### Data Preprocessing
```python
from preprocessing.preprocess_pipeline import preprocess_pipeline

X_train, X_test, y_train, y_test = preprocess_pipeline(
    filepath='backend/data/raw/KDDTrain+.txt',
    target_column='Label',
    test_size=0.2,
    normalization_method='standard'
)
```

### Feature Selection
```python
from features.feature_selection import select_kbest_features

# Select top 20 features
selected_indices = select_kbest_features(X_train, y_train, k=20)
X_train_selected = X_train[:, selected_indices]
X_test_selected = X_test[:, selected_indices]
```

### Model Training
```python
from models.train_model import XIDSTrainer

trainer = XIDSTrainer()
metrics = trainer.train_pipeline('backend/data/processed/train_processed.csv')
print(f"F1-Score: {metrics['f1_score']:.4f}")
```

### Model Explanation
```python
from explainability.shap_explainer import SHAPExplainer

explainer = SHAPExplainer(model, X_train, model_type='tree')
explanation = explainer.explain_instance(X_test[0])
print(f"Base Value: {explanation['base_value']}")
print(f"SHAP Values: {explanation['shap_values']}")
```

## ğŸ“š Jupyter Notebooks

### data_exploration.ipynb
- Dataset overview
- Statistical analysis
- Class distribution
- Feature characteristics
- Data quality assessment

### feature_importance.ipynb
- Feature importance ranking
- Feature selection methods
- Correlation analysis
- Cumulative importance
- Selection recommendations

## ğŸ”§ Configuration

Edit `backend/utils/config.py` to customize:

```python
# Model hyperparameters
MODEL_CONFIG = {
    'xgboost': {
        'n_estimators': 100,
        'max_depth': 8,
        'learning_rate': 0.1,
    },
    'random_forest': {
        'n_estimators': 100,
        'max_depth': 15,
    }
}

# Feature selection
FEATURE_CONFIG = {
    'selection_method': 'forest',  # 'kbest', 'mutual_info', 'forest'
    'num_features': 20,
    'outlier_threshold': 3.0
}
```

## ğŸ³ Docker Deployment

```bash
# Build and run with Docker Compose
docker-compose up --build

# Backend will be at http://localhost:8000
# Frontend will be at http://localhost:8501
```

## ğŸ“Š Results Management

All outputs are saved in the `results/` directory:

```
results/
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ model_performance.json
â”‚   â”œâ”€â”€ confusion_matrix.csv
â”‚   â””â”€â”€ classification_report.txt
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â””â”€â”€ roc_curve.png
â””â”€â”€ explanations/
    â”œâ”€â”€ shap_explanations.json
    â”œâ”€â”€ lime_explanations.json
    â””â”€â”€ rule_extractions.json
```

## ğŸ” Security & Authentication

- **Login System**: Email/password authentication
- **User Database**: Hashed credentials in `.users.json`
- **Session Management**: Streamlit session state
- **Password Hashing**: SHA-256 encryption

## ğŸ“ˆ Performance Monitoring

- Model metrics saved in `results/metrics/`
- Predictions logged for analysis
- Explanation generation timestamped
- Performance trends tracked

## ğŸš€ Development Workflow

1. **Exploration**: Use Jupyter notebooks
2. **Development**: Edit modules in backend/
3. **Testing**: Run train_model.py
4. **Validation**: Test via API and frontend
5. **Deployment**: Use Docker Compose

## ğŸ“– Documentation

- **PROJECT_STRUCTURE.md**: Detailed structure guide
- **README.md**: Project overview
- **DOCUMENTATION.md**: Complete documentation
- **Inline Comments**: Code-level documentation

## ğŸ¤ Contributing

When contributing:
1. Follow the modular structure
2. Add tests for new modules
3. Update configuration in `config.py`
4. Document functions and classes
5. Update notebooks for major changes

## ğŸ“ License

This project is part of the XIDS (Explainable Intrusion Detection System) initiative.

## ğŸ†˜ Troubleshooting

### Model File Not Found
```bash
cd backend/models
python train_model.py
```

### Port Already in Use
```bash
# Kill process on port 8000
lsof -ti:8000 | xargs kill -9

# Or use different port
python -m uvicorn app.main:app --port 8001
```

### Missing Dependencies
```bash
pip install -r requirements.txt
pip install -r backend/requirements.txt
pip install -r frontend/requirements.txt
```

## ğŸ“ Support

For issues or questions:
1. Check the notebooks for examples
2. Review configuration in `config.py`
3. Check API documentation at `/docs`
4. Review logs in `results/` folder

---

**Last Updated**: February 15, 2026  
**Version**: 2.0 (Reorganized Structure)
