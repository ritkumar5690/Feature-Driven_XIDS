{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4094dbcc",
   "metadata": {},
   "source": [
    "# XIDS Feature Importance Analysis Notebook\n",
    "\n",
    "This notebook analyzes feature importance using various techniques to identify the most relevant features for intrusion detection.\n",
    "\n",
    "## Contents:\n",
    "1. Feature Importance from Tree-Based Models\n",
    "2. Feature Selection Methods\n",
    "3. Correlation Analysis\n",
    "4. Feature Importance Visualization\n",
    "5. Feature Selection Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5e912",
   "metadata": {},
   "source": [
    "## 1. Feature Importance from Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up visualization\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "# Define paths\n",
    "BACKEND_DIR = Path('../backend')\n",
    "DATA_DIR = BACKEND_DIR / 'data' / 'raw'\n",
    "\n",
    "print(\"Feature Importance Analysis for XIDS\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0790ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "try:\n",
    "    train_file = DATA_DIR / 'KDDTrain+.txt'\n",
    "    df = pd.read_csv(train_file, nrows=5000)\n",
    "    print(f\"Data loaded: {df.shape}\")\n",
    "except:\n",
    "    print(\"Using synthetic data for demonstration\")\n",
    "    n_samples = 1000\n",
    "    n_features = 41\n",
    "    df = pd.DataFrame(np.random.randn(n_samples, n_features))\n",
    "    # Add a synthetic label\n",
    "    df['label'] = np.random.choice(['BENIGN', 'Attack'], n_samples)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Column names (first 5): {list(df.columns[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38266b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "label_col = df.columns[-1]\n",
    "X = df.drop(columns=[label_col])\n",
    "y = df[label_col]\n",
    "\n",
    "# Convert labels to numeric if needed\n",
    "if y.dtype == 'object':\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0256de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest for feature importance\n",
    "print(\"Training Random Forest for feature importance...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "print(feature_importance_rf.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de8c2c",
   "metadata": {},
   "source": [
    "## 2. Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectKBest with F-score\n",
    "print(\"SelectKBest Feature Selection (F-score)...\")\n",
    "select_kbest = SelectKBest(score_func=f_classif, k=20)\n",
    "select_kbest.fit(X, y)\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_indices = select_kbest.get_support(indices=True)\n",
    "selected_features_kbest = X.columns[selected_indices].tolist()\n",
    "\n",
    "print(f\"Selected features: {len(selected_features_kbest)}\")\n",
    "print(f\"Features: {selected_features_kbest[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c032e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual Information\n",
    "print(\"\\nMutual Information Feature Selection...\")\n",
    "select_mi = SelectKBest(score_func=mutual_info_classif, k=20)\n",
    "select_mi.fit(X, y)\n",
    "\n",
    "# Get MI scores\n",
    "mi_scores = select_mi.scores_\n",
    "mi_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'score': mi_scores\n",
    "}).sort_values('score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Features (Mutual Information):\")\n",
    "print(mi_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c300835a",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "print(\"Computing correlations...\")\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "X_numeric = X[numeric_cols]\n",
    "\n",
    "if X_numeric.shape[1] > 0:\n",
    "    correlations = X_numeric.corrwith(y).abs().sort_values(ascending=False)\n",
    "    print(\"\\nTop 10 Features by Correlation with Target:\")\n",
    "    print(correlations.head(10))\n",
    "else:\n",
    "    print(\"No numeric features for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dae51a",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features from Random Forest\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Random Forest - Top 15 features\n",
    "top_n = 15\n",
    "top_features_rf = feature_importance_rf.head(top_n)\n",
    "axes[0, 0].barh(range(top_n), top_features_rf['importance'], color='steelblue')\n",
    "axes[0, 0].set_yticks(range(top_n))\n",
    "axes[0, 0].set_yticklabels(top_features_rf['feature'])\n",
    "axes[0, 0].set_xlabel('Importance Score')\n",
    "axes[0, 0].set_title('Top 15 Features - Random Forest Importance', fontweight='bold')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# 2. Feature importance distribution\n",
    "axes[0, 1].hist(feature_importance_rf['importance'], bins=30, color='steelblue', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Importance Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Feature Importances', fontweight='bold')\n",
    "\n",
    "# 3. Mutual Information - Top 15 features\n",
    "if len(mi_importance) > 0:\n",
    "    top_features_mi = mi_importance.head(top_n)\n",
    "    axes[1, 0].barh(range(min(top_n, len(top_features_mi))), \n",
    "                    top_features_mi['score'][:top_n], color='coral')\n",
    "    axes[1, 0].set_yticks(range(min(top_n, len(top_features_mi))))\n",
    "    axes[1, 0].set_yticklabels(top_features_mi['feature'][:top_n])\n",
    "    axes[1, 0].set_xlabel('MI Score')\n",
    "    axes[1, 0].set_title('Top 15 Features - Mutual Information', fontweight='bold')\n",
    "    axes[1, 0].invert_yaxis()\n",
    "\n",
    "# 4. Cumulative importance\n",
    "cumulative_importance = np.cumsum(feature_importance_rf['importance'].values)\n",
    "axes[1, 1].plot(range(len(cumulative_importance)), cumulative_importance, marker='o', linewidth=2)\n",
    "axes[1, 1].axhline(y=0.8, color='red', linestyle='--', label='80% threshold')\n",
    "axes[1, 1].axhline(y=0.9, color='orange', linestyle='--', label='90% threshold')\n",
    "axes[1, 1].set_xlabel('Number of Features')\n",
    "axes[1, 1].set_ylabel('Cumulative Importance')\n",
    "axes[1, 1].set_title('Cumulative Feature Importance', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282d3fc",
   "metadata": {},
   "source": [
    "## 5. Feature Selection Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1800f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance distribution\n",
    "print(\"Feature Importance Analysis:\")\n",
    "print(f\"Total features: {len(feature_importance_rf)}\")\n",
    "print(f\"Mean importance: {feature_importance_rf['importance'].mean():.6f}\")\n",
    "print(f\"Std importance: {feature_importance_rf['importance'].std():.6f}\")\n",
    "\n",
    "# Find number of features for 80% and 90% importance\n",
    "cumulative = np.cumsum(feature_importance_rf['importance'].values)\n",
    "features_80 = np.argmax(cumulative >= 0.8) + 1\n",
    "features_90 = np.argmax(cumulative >= 0.9) + 1\n",
    "\n",
    "print(f\"\\nFeatures needed for:\")\n",
    "print(f\"  80% importance: {features_80} features\")\n",
    "print(f\"  90% importance: {features_90} features\")\n",
    "print(f\"  95% importance: {np.argmax(cumulative >= 0.95) + 1} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE SELECTION RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. For Balanced Performance & Speed:\")\n",
    "print(f\"   - Select top {features_80} features\")\n",
    "print(f\"   - Explanation: Captures 80% of feature importance\")\n",
    "print(f\"   - Top features: {list(feature_importance_rf['feature'].head(features_80))}\")\n",
    "\n",
    "print(f\"\\n2. For High Accuracy:\")\n",
    "print(f\"   - Select top {features_90} features\")\n",
    "print(f\"   - Explanation: Captures 90% of feature importance\")\n",
    "print(f\"   - Top features: {list(feature_importance_rf['feature'].head(features_90))}\")\n",
    "\n",
    "print(f\"\\n3. For Model Interpretability:\")\n",
    "print(f\"   - Use top 10-15 features\")\n",
    "print(f\"   - Easier to understand and visualize\")\n",
    "print(f\"   - Top 15: {list(feature_importance_rf['feature'].head(15))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b3a60",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis provides:\n",
    "1. **Feature Importance Ranking**: Based on Random Forest trained on the dataset\n",
    "2. **Feature Selection Methods**: Multiple techniques to identify relevant features\n",
    "3. **Cumulative Analysis**: Shows diminishing returns of adding more features\n",
    "4. **Recommendations**: Actionable guidance for feature selection\n",
    "\n",
    "### Key Insights:\n",
    "- A subset of features captures most of the predictive power\n",
    "- Dimensionality reduction improves model efficiency\n",
    "- Different methods may prioritize different features\n",
    "- Balance between accuracy and interpretability is important\n",
    "\n",
    "### Next Steps:\n",
    "1. Apply selected features to train the final model\n",
    "2. Use explainability techniques to understand predictions\n",
    "3. Validate feature selection on test set\n",
    "4. Monitor feature importance in production"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
